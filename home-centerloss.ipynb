{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"},"colab":{"name":"home-centerloss.ipynb","provenance":[],"collapsed_sections":["5dLFUMtavvMq","Dl9AVuYVvvNe","qtTlLAljvvNg"],"machine_shape":"hm"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"lJ5tOQlBvvIE","colab_type":"text"},"source":["# Bengali.AI SEResNeXt training with pytorch\n","\n","I will introduce following contents\n","\n"," - **Fast data loading** with feather format\n"," - **Data augmentation** technic with affine transformation\n"," - **CNN SoTA models**: Use pytorch `pretrainedmodels` library, especially I use **`SEResNeXt`** in this notebook\n"," - **Training code abstraction**: Use `pytorch-ignite` module for the trainining abstraction\n"," \n","### Update history\n","\n"," - 2020/1/4 v2: Added albumentations augmentations introduced in [Bengali: albumentations data augmentation tutorial](https://www.kaggle.com/corochann/bengali-albumentations-data-augmentation-tutorial)"]},{"cell_type":"markdown","metadata":{"id":"IkYX9tP6vvII","colab_type":"text"},"source":["# Table of Contents:\n","**[Fast data loading with feather](#load)**<br>\n","**[Dataset](#dataset)**<br>\n","**[Data augmentation/processing](#processing)**<br>\n","**[pytorch model & define classifier](#model)**<br>\n","**[Training code](#train)**<br>\n","**[Prediction](#pred)**<br>\n","**[Reference and further reading](#ref)**<br>"]},{"cell_type":"markdown","metadata":{"id":"hSG-2_JuvvIU","colab_type":"text"},"source":["To install https://github.com/Cadene/pretrained-models.pytorch without internet connection, we can install library as \"dataset\".\n","\n","It is uploaded by @rishabhiitbhu : https://www.kaggle.com/rishabhiitbhu/pretrainedmodels"]},{"cell_type":"code","metadata":{"id":"-HrP-O-sa_gp","colab_type":"code","outputId":"0e8d739c-e07c-41fe-82ec-a8ac28152e39","executionInfo":{"status":"ok","timestamp":1583972652717,"user_tz":-540,"elapsed":28742,"user":{"displayName":"K Rie","photoUrl":"","userId":"01922819648522133437"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["from google.colab import drive\n","drive.mount(mountpoint=\"content\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at content\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"id":"oyR2xpPWvvIW","colab_type":"code","colab":{}},"source":["import gc\n","import os\n","from pathlib import Path\n","import random\n","import sys\n","\n","from tqdm import tqdm\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","from IPython.core.display import display, HTML\n","\n","# --- plotly ---\n","#from plotly import tools, subplots\n","#import plotly.offline as py\n","#py.init_notebook_mode(connected=True)\n","#import plotly.graph_objs as go\n","#import plotly.express as px\n","#import plotly.figure_factory as ff\n","\n","# --- models ---\n","from sklearn import preprocessing\n","from sklearn.model_selection import KFold\n","\n","\n","# --- setup ---\n","pd.set_option('max_columns', 50)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lZaIZv8Ck1jc","colab_type":"code","outputId":"212f3318-b627-476b-f26e-069d54253dc1","executionInfo":{"status":"ok","timestamp":1583972687555,"user_tz":-540,"elapsed":20859,"user":{"displayName":"K Rie","photoUrl":"","userId":"01922819648522133437"}},"colab":{"base_uri":"https://localhost:8080/","height":680}},"source":["!pip install -U git+https://github.com/albu/albumentations --no-cache-dir"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting git+https://github.com/albu/albumentations\n","  Cloning https://github.com/albu/albumentations to /tmp/pip-req-build-44opyi6c\n","  Running command git clone -q https://github.com/albu/albumentations /tmp/pip-req-build-44opyi6c\n","Requirement already satisfied, skipping upgrade: numpy>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from albumentations==0.4.5) (1.17.5)\n","Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from albumentations==0.4.5) (1.4.1)\n","Collecting imgaug<0.2.7,>=0.2.5\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/2e/748dbb7bb52ec8667098bae9b585f448569ae520031932687761165419a2/imgaug-0.2.6.tar.gz (631kB)\n","\u001b[K     |████████████████████████████████| 634kB 2.8MB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: PyYAML in /usr/local/lib/python3.6/dist-packages (from albumentations==0.4.5) (3.13)\n","Requirement already satisfied, skipping upgrade: opencv-python>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from albumentations==0.4.5) (4.1.2.30)\n","Requirement already satisfied, skipping upgrade: scikit-image>=0.11.0 in /usr/local/lib/python3.6/dist-packages (from imgaug<0.2.7,>=0.2.5->albumentations==0.4.5) (0.16.2)\n","Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from imgaug<0.2.7,>=0.2.5->albumentations==0.4.5) (1.12.0)\n","Requirement already satisfied, skipping upgrade: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.5) (3.1.3)\n","Requirement already satisfied, skipping upgrade: imageio>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.5) (2.4.1)\n","Requirement already satisfied, skipping upgrade: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.5) (1.1.1)\n","Requirement already satisfied, skipping upgrade: pillow>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.5) (6.2.2)\n","Requirement already satisfied, skipping upgrade: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.5) (2.4)\n","Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.5) (0.10.0)\n","Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.5) (2.4.6)\n","Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.5) (2.6.1)\n","Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.5) (1.1.0)\n","Requirement already satisfied, skipping upgrade: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.5) (4.4.1)\n","Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.5) (45.2.0)\n","Building wheels for collected packages: albumentations, imgaug\n","  Building wheel for albumentations (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for albumentations: filename=albumentations-0.4.5-cp36-none-any.whl size=64514 sha256=e0bc39937e5d7c42a1c4e158bc408f2549a8b69f49ef55f5a4180891bd3de2ca\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-lgmwlomg/wheels/45/8b/e4/2837bbcf517d00732b8e394f8646f22b8723ac00993230188b\n","  Building wheel for imgaug (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for imgaug: filename=imgaug-0.2.6-cp36-none-any.whl size=654020 sha256=c8cec7c7cd81d3038331bc335bf14da6106f75bee7596701aa40f8842b3dded9\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-lgmwlomg/wheels/97/ec/48/0d25896c417b715af6236dbcef8f0bed136a1a5e52972fc6d0\n","Successfully built albumentations imgaug\n","Installing collected packages: imgaug, albumentations\n","  Found existing installation: imgaug 0.2.9\n","    Uninstalling imgaug-0.2.9:\n","      Successfully uninstalled imgaug-0.2.9\n","  Found existing installation: albumentations 0.1.12\n","    Uninstalling albumentations-0.1.12:\n","      Successfully uninstalled albumentations-0.1.12\n","Successfully installed albumentations-0.4.5 imgaug-0.2.6\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"36L_kTwPvvIg","colab_type":"code","outputId":"9ac8f239-91ab-4385-87c7-de7652c6439b","executionInfo":{"status":"ok","timestamp":1583972689172,"user_tz":-540,"elapsed":22462,"user":{"displayName":"K Rie","photoUrl":"","userId":"01922819648522133437"}},"colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["!git clone https://github.com/KaiyangZhou/pytorch-center-loss"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Cloning into 'pytorch-center-loss'...\n","remote: Enumerating objects: 212, done.\u001b[K\n","Receiving objects:   0% (1/212)   \rReceiving objects:   1% (3/212)   \rReceiving objects:   2% (5/212)   \rReceiving objects:   3% (7/212)   \rReceiving objects:   4% (9/212)   \rReceiving objects:   5% (11/212)   \rReceiving objects:   6% (13/212)   \rReceiving objects:   7% (15/212)   \rReceiving objects:   8% (17/212)   \rReceiving objects:   9% (20/212)   \rReceiving objects:  10% (22/212)   \rReceiving objects:  11% (24/212)   \rReceiving objects:  12% (26/212)   \rReceiving objects:  13% (28/212)   \rReceiving objects:  14% (30/212)   \rReceiving objects:  15% (32/212)   \rReceiving objects:  16% (34/212)   \rReceiving objects:  17% (37/212)   \rReceiving objects:  18% (39/212)   \rReceiving objects:  19% (41/212)   \rReceiving objects:  20% (43/212)   \rReceiving objects:  21% (45/212)   \rReceiving objects:  22% (47/212)   \rReceiving objects:  23% (49/212)   \rReceiving objects:  24% (51/212)   \rReceiving objects:  25% (53/212)   \rReceiving objects:  26% (56/212)   \rReceiving objects:  27% (58/212)   \rReceiving objects:  28% (60/212)   \rReceiving objects:  29% (62/212)   \rReceiving objects:  30% (64/212)   \rReceiving objects:  31% (66/212)   \rReceiving objects:  32% (68/212)   \rReceiving objects:  33% (70/212)   \rReceiving objects:  34% (73/212)   \rReceiving objects:  35% (75/212)   \rReceiving objects:  36% (77/212)   \rReceiving objects:  37% (79/212)   \rReceiving objects:  38% (81/212)   \rremote: Total 212 (delta 0), reused 0 (delta 0), pack-reused 212\u001b[K\n","Receiving objects:  39% (83/212)   \rReceiving objects:  40% (85/212)   \rReceiving objects:  41% (87/212)   \rReceiving objects:  42% (90/212)   \rReceiving objects:  43% (92/212)   \rReceiving objects:  44% (94/212)   \rReceiving objects:  45% (96/212)   \rReceiving objects:  46% (98/212)   \rReceiving objects:  47% (100/212)   \rReceiving objects:  48% (102/212)   \rReceiving objects:  49% (104/212)   \rReceiving objects:  50% (106/212)   \rReceiving objects:  51% (109/212)   \rReceiving objects:  52% (111/212)   \rReceiving objects:  53% (113/212)   \rReceiving objects:  54% (115/212)   \rReceiving objects:  55% (117/212)   \rReceiving objects:  56% (119/212)   \rReceiving objects:  57% (121/212)   \rReceiving objects:  58% (123/212)   \rReceiving objects:  59% (126/212)   \rReceiving objects:  60% (128/212)   \rReceiving objects:  61% (130/212)   \rReceiving objects:  62% (132/212)   \rReceiving objects:  63% (134/212)   \rReceiving objects:  64% (136/212)   \rReceiving objects:  65% (138/212)   \rReceiving objects:  66% (140/212)   \rReceiving objects:  67% (143/212)   \rReceiving objects:  68% (145/212)   \rReceiving objects:  69% (147/212)   \rReceiving objects:  70% (149/212)   \rReceiving objects:  71% (151/212)   \rReceiving objects:  72% (153/212)   \rReceiving objects:  73% (155/212)   \rReceiving objects:  74% (157/212)   \rReceiving objects:  75% (159/212)   \rReceiving objects:  76% (162/212)   \rReceiving objects:  77% (164/212)   \rReceiving objects:  78% (166/212)   \rReceiving objects:  79% (168/212)   \rReceiving objects:  80% (170/212)   \rReceiving objects:  81% (172/212)   \rReceiving objects:  82% (174/212)   \rReceiving objects:  83% (176/212)   \rReceiving objects:  84% (179/212)   \rReceiving objects:  85% (181/212)   \rReceiving objects:  86% (183/212)   \rReceiving objects:  87% (185/212)   \rReceiving objects:  88% (187/212)   \rReceiving objects:  89% (189/212)   \rReceiving objects:  90% (191/212)   \rReceiving objects:  91% (193/212)   \rReceiving objects:  92% (196/212)   \rReceiving objects:  93% (198/212)   \rReceiving objects:  94% (200/212)   \rReceiving objects:  95% (202/212)   \rReceiving objects:  96% (204/212)   \rReceiving objects:  97% (206/212)   \rReceiving objects:  98% (208/212)   \rReceiving objects:  99% (210/212)   \rReceiving objects: 100% (212/212)   \rReceiving objects: 100% (212/212), 5.67 MiB | 34.76 MiB/s, done.\n","Resolving deltas:   0% (0/130)   \rResolving deltas:   3% (5/130)   \rResolving deltas:  61% (80/130)   \rResolving deltas:  63% (82/130)   \rResolving deltas:  73% (96/130)   \rResolving deltas:  80% (104/130)   \rResolving deltas:  82% (107/130)   \rResolving deltas:  83% (108/130)   \rResolving deltas:  98% (128/130)   \rResolving deltas: 100% (130/130)   \rResolving deltas: 100% (130/130), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"PlUpc-dHvvIk","colab_type":"code","outputId":"faf1fdf6-a596-4df1-c775-4523d8c5cd50","executionInfo":{"status":"ok","timestamp":1583972689174,"user_tz":-540,"elapsed":22438,"user":{"displayName":"K Rie","photoUrl":"","userId":"01922819648522133437"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["cd pytorch-center-loss\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/content/pytorch-center-loss\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LSX0rAIxvvIo","colab_type":"code","colab":{}},"source":["from center_loss import CenterLoss\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Gyyex0XvvvIs","colab_type":"code","outputId":"1be8fac7-d4f4-45b0-bd93-5b4b292a192f","executionInfo":{"status":"ok","timestamp":1583972692163,"user_tz":-540,"elapsed":25334,"user":{"displayName":"K Rie","photoUrl":"","userId":"01922819648522133437"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["cd .."],"execution_count":0,"outputs":[{"output_type":"stream","text":["/content\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"oN2OdYetvvI1","colab_type":"text"},"source":["# Config"]},{"cell_type":"code","metadata":{"id":"R6FrOSGYvvI2","colab_type":"code","colab":{}},"source":["debug=False\n","submission=False\n","batch_size=128\n","device='cuda:0'\n","out='.'\n","image_size=64\n","n_splits = 5\n","arch='pretrained'\n","model_name='se_resnext101_32x4d'\n","model_name='se_resnext50_32x4d'\n","load_model_name = False\n","load_model_name = 'predictorfold0_centercutout_001.pt'\n","saved_model_name = 'predictorfold0_centercutout_001.pt'\n","seed = 42\n","n_splits = 5\n","pretrained = \"imagenet\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_kg_hide-input":false,"_kg_hide-output":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","id":"DTe-cEB4vvI8","colab_type":"code","outputId":"f63b4f32-9f4f-45f6-d383-c79872aa8cba","executionInfo":{"status":"ok","timestamp":1583972692735,"user_tz":-540,"elapsed":25833,"user":{"displayName":"K Rie","photoUrl":"","userId":"01922819648522133437"}},"colab":{"base_uri":"https://localhost:8080/","height":374}},"source":["%cd /content/content/My Drive/bengali-ai/kaggle/working\n","\n","input_path = \"/content/content/My Drive/bengali-ai/kaggle/input/\"\n","work_path = \"/content/content/My Drive/bengali-ai/kaggle/working/\"\n","datadir = Path(input_path+'bengaliai-cv19')\n","featherdir = Path(input_path+'bengaliaicv19feather')\n","outdir = work_path \n","load_model_path = outdir +\"models/\"+ load_model_name if load_model_name else False\n","for dirname, _, filenames in os.walk(input_path):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/content/content/My Drive/bengali-ai/kaggle/working\n","/content/content/My Drive/bengali-ai/kaggle/input/bengaliaicv19feather/test_image_data_0.feather\n","/content/content/My Drive/bengali-ai/kaggle/input/bengaliaicv19feather/test_image_data_1.feather\n","/content/content/My Drive/bengali-ai/kaggle/input/bengaliaicv19feather/test_image_data_2.feather\n","/content/content/My Drive/bengali-ai/kaggle/input/bengaliaicv19feather/test_image_data_3.feather\n","/content/content/My Drive/bengali-ai/kaggle/input/bengaliaicv19feather/train_image_data_0.feather\n","/content/content/My Drive/bengali-ai/kaggle/input/bengaliaicv19feather/train_image_data_1.feather\n","/content/content/My Drive/bengali-ai/kaggle/input/bengaliaicv19feather/train_image_data_2.feather\n","/content/content/My Drive/bengali-ai/kaggle/input/bengaliaicv19feather/train_image_data_3.feather\n","/content/content/My Drive/bengali-ai/kaggle/input/bengaliai-cv19/class_map.csv\n","/content/content/My Drive/bengali-ai/kaggle/input/bengaliai-cv19/sample_submission.csv\n","/content/content/My Drive/bengali-ai/kaggle/input/bengaliai-cv19/test.csv\n","/content/content/My Drive/bengali-ai/kaggle/input/bengaliai-cv19/test_image_data_0.parquet\n","/content/content/My Drive/bengali-ai/kaggle/input/bengaliai-cv19/test_image_data_1.parquet\n","/content/content/My Drive/bengali-ai/kaggle/input/bengaliai-cv19/test_image_data_2.parquet\n","/content/content/My Drive/bengali-ai/kaggle/input/bengaliai-cv19/test_image_data_3.parquet\n","/content/content/My Drive/bengali-ai/kaggle/input/bengaliai-cv19/train.csv\n","/content/content/My Drive/bengali-ai/kaggle/input/bengaliai-cv19/train_image_data_0.parquet\n","/content/content/My Drive/bengali-ai/kaggle/input/bengaliai-cv19/train_image_data_1.parquet\n","/content/content/My Drive/bengali-ai/kaggle/input/bengaliai-cv19/train_image_data_2.parquet\n","/content/content/My Drive/bengali-ai/kaggle/input/bengaliai-cv19/train_image_data_3.parquet\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"id":"m_qZzPXjvvJG","colab_type":"code","colab":{}},"source":["# Read in the data CSV files\n","# train = pd.read_csv(datadir/'train.csv')\n","# test = pd.read_csv(datadir/'test.csv')\n","# sample_submission = pd.read_csv(datadir/'sample_submission.csv')\n","# class_map = pd.read_csv(datadir/'class_map.csv')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KnUa1J9KvvJM","colab_type":"text"},"source":["# Fast data loading with feather\n","\n","Refer [Bengali.AI super fast data loading with feather](https://www.kaggle.com/corochann/bengali-ai-super-fast-data-loading-with-feather) and [dataset](https://www.kaggle.com/corochann/bengaliaicv19feather) for detail.<br/>\n","Original `parquet` format takes about 60 sec to load 1 data, while `feather` format takes about **2 sec to load 1 data!!!**\n","\n","### How to add dataset\n","\n","When you write kernel, click \"+ Add Data\" botton on right top.<br/>\n","Then inside window pop-up, you can see \"Search Datasets\" text box on right top.<br/>\n","You can type \"bengaliai-cv19-feather\" to find this dataset and press \"Add\" botton to add the data."]},{"cell_type":"code","metadata":{"id":"jtu8ItO1vvJN","colab_type":"code","colab":{}},"source":["#画像前処理\n","#feather data から　画像データ"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uHVSv0HtvvJQ","colab_type":"code","colab":{}},"source":["import numpy as np\n","import pandas as pd\n","import gc\n","\n","\n","def prepare_image(datadir, featherdir, data_type='train',\n","                  submission=False, indices=[0, 1, 2, 3]):\n","    assert data_type in ['train', 'test']\n","    #parquet or feather\n","    if submission:\n","        image_df_list = [pd.read_parquet(datadir / f'{data_type}_image_data_{i}.parquet')\n","                         for i in indices]\n","    else:\n","        image_df_list = [pd.read_feather(featherdir / f'{data_type}_image_data_{i}.feather')\n","                         for i in indices]\n","\n","    print('image_df_list', len(image_df_list))\n","    HEIGHT = 137\n","    WIDTH = 236\n","    \n","    images = [df.iloc[:, 1:].values.reshape(-1, HEIGHT, WIDTH) for df in image_df_list]\n","    del image_df_list\n","    gc.collect()\n","    #feather data は４つあるのでconcate\n","    images = np.concatenate(images, axis=0)\n","    return images\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nWLmC52-vvJW","colab_type":"code","colab":{}},"source":["# os.chdir('/home/chikazoe/operation/bengali/')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MnHeAb3qvvJa","colab_type":"code","outputId":"f4215a16-ef1d-45f7-e10a-16a77100c7d2","executionInfo":{"status":"ok","timestamp":1583929879004,"user_tz":-540,"elapsed":241406,"user":{"displayName":"K Rie","photoUrl":"","userId":"01922819648522133437"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["%%time\n","train = pd.read_csv(datadir/'train.csv')\n","train_labels = train[['grapheme_root', 'vowel_diacritic', 'consonant_diacritic']].values\n","indices = [0] if debug else [0, 1, 2, 3]\n","train_images = prepare_image(\n","    datadir, featherdir, data_type='train', submission=submission, indices=indices)\n","# train_images.shape (50210, 137, 236) debug\n","# train_images.shape (200840, 137, 236)\"\"\"\n","#train_fold = pd.read_csv(datadir/'train_with_fold.csv')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["image_df_list 4\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TlLsfGj3vvJm","colab_type":"code","colab":{}},"source":["def bbox(img):\n","    rows = np.any(img, axis=1)\n","    cols = np.any(img, axis=0)\n","    rmin, rmax = np.where(rows)[0][[0, -1]]\n","    cmin, cmax = np.where(cols)[0][[0, -1]]\n","    return rmin, rmax, cmin, cmax\n","import cv2\n","def crop_resize(img0, size=128, pad=16):\n","    #crop a box around pixels large than the threshold \n","    #some images contain line at the sides\n","    HEIGHT = 137\n","    WIDTH = 236\n","    img0 = 255 - img0\n","    img0 = (img0*(255.0/img0.max())).astype(np.uint8)\n","    ymin,ymax,xmin,xmax = bbox(img0[5:-5,5:-5] > 80)\n","    #cropping may cut too much, so we need to add it back\n","    xmin = xmin - 13 if (xmin > 13) else 0\n","    ymin = ymin - 10 if (ymin > 10) else 0\n","    xmax = xmax + 13 if (xmax < WIDTH - 13) else WIDTH\n","    ymax = ymax + 10 if (ymax < HEIGHT - 10) else HEIGHT\n","    img = img0[ymin:ymax,xmin:xmax]\n","    #remove lo intensity pixels as noise\n","    img[img < 28] = 0\n","    lx, ly = xmax-xmin,ymax-ymin\n","    l = max(lx,ly) + pad\n","    #make sure that the aspect ratio is kept in rescaling\n","    img = np.pad(img, [((l-ly)//2,), ((l-lx)//2,)], mode='constant')\n","    return cv2.resize(img,(size,size))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5fZJqsL-vvJr","colab_type":"code","colab":{}},"source":["#A = list()\n","#for i in range(len(train_images)):\n","#    A.append(crop_resize(train_images[i],128)[None])\n","#data = np.load(\"train128.npz\")\n","#data['arr_0'].shape\n","# train_images = np.load(\"train128.npz\")['arr_0']"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ApdNUI-6vvJ7","colab_type":"text"},"source":["Let's see how this `BengaliAIDataset` work"]},{"cell_type":"code","metadata":{"id":"HJBAMJmrvvKB","colab_type":"code","colab":{}},"source":["\"\"\"\n","Referenced `chainer.dataset.DatasetMixin` to work with pytorch Dataset.\n","\"\"\"\n","import numpy\n","import six\n","import torch\n","from torch.utils.data.dataset import Dataset\n","\n","\n","class DatasetMixin(Dataset):\n","\n","    def __init__(self, transform=None):\n","        self.transform = transform\n","\n","    def __getitem__(self, index):\n","        \"\"\"Returns an example or a sequence of examples.\"\"\"\n","        if torch.is_tensor(index):\n","            index = index.tolist()\n","        if isinstance(index, slice):\n","            current, stop, step = index.indices(len(self))\n","            return [self.get_example_wrapper(i) for i in\n","                    six.moves.range(current, stop, step)]\n","        elif isinstance(index, list) or isinstance(index, numpy.ndarray):\n","            return [self.get_example_wrapper(i) for i in index]\n","        else:\n","            return self.get_example_wrapper(index)\n","\n","    def __len__(self):\n","        \"\"\"Returns the number of data points.\"\"\"\n","        raise NotImplementedError\n","\n","    def get_example_wrapper(self, i):\n","        \"\"\"Wrapper of `get_example`, to apply `transform` if necessary\"\"\"\n","        example = self.get_example(i)\n","        if self.transform:\n","            example = self.transform(example)\n","        return example\n","\n","    def get_example(self, i):\n","        \"\"\"Returns the i-th example.\n","\n","        Implementations should override it. It should raise :class:`IndexError`\n","        if the index is invalid.\n","\n","        Args:\n","            i (int): The index of the example.\n","\n","        Returns:\n","            The i-th example.\n","\n","        \"\"\"\n","        raise NotImplementedError\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6R0wOEg9vvKF","colab_type":"code","colab":{}},"source":["import numpy as np\n","\n","\n","class BengaliAIDataset(DatasetMixin):\n","    def __init__(self, images, labels=None, transform=None, indices=None):\n","        super(BengaliAIDataset, self).__init__(transform=transform)\n","        self.images = images\n","        self.labels = labels\n","        if indices is None:\n","            indices = np.arange(len(images))\n","        self.indices = indices\n","        self.train = labels is not None\n","\n","    def __len__(self):\n","        \"\"\"return length of this dataset\"\"\"\n","        return len(self.indices)\n","\n","    def get_example(self, i):\n","        \"\"\"Return i-th data\"\"\"\n","        i = self.indices[i]\n","        x = self.images[i]\n","        # Opposite white and black: background will be white and\n","        # for future Affine transformation\n","        x = (255 - x).astype(np.float32) / 255.\n","        if self.train:\n","            y = self.labels[i]\n","            return x, y\n","        else:\n","            return x\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"66IrWUNcvvKI","colab_type":"code","colab":{}},"source":["train_dataset = BengaliAIDataset(train_images, train_labels)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H5NN33RxvvKM","colab_type":"text"},"source":["`train_dataset[i]` returns i-th image array and 3 target labels (graphme_root, vowel_diacritic and consonant_diacritic)."]},{"cell_type":"code","metadata":{"id":"MkChTk-jvvKN","colab_type":"code","colab":{}},"source":["image, label = train_dataset[0]\n","print('image', image.shape, 'label', label)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3ykOqq2BvvKP","colab_type":"code","colab":{}},"source":["train_dataset.images.shape"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SuFUOdD4vvKb","colab_type":"text"},"source":["<a id=\"processing\"></a>\n","# Data augmentation/processing"]},{"cell_type":"markdown","metadata":{"id":"KoCuIaVhvvKb","colab_type":"text"},"source":["For CNN training, data augmentation is important to improve test accuracy (generalization performance). I will show some image preprocessing to increase the data variety."]},{"cell_type":"code","metadata":{"_kg_hide-input":true,"id":"MgFC45ICvvKc","colab_type":"code","colab":{}},"source":["\"\"\"\n","From https://www.kaggle.com/corochann/deep-learning-cnn-with-chainer-lb-0-99700\n","\"\"\"\n","import cv2\n","from skimage.transform import AffineTransform, warp\n","import numpy as np\n","\n","\n","def affine_image(img):\n","    \"\"\"\n","    アフィン変換\n","    max_shear_angleとかmax_scaleがどの程度が最適化かは検討もつかない\n","    Args:\n","        img: (h, w) or (1, h, w)\n","\n","    Returns:\n","        img: (h, w)\n","    \"\"\"\n","    # ch, h, w = img.shape\n","    # img = img / 255.\n","    if img.ndim == 3:\n","        img = img[0]\n","\n","    # --- scale ---\n","    min_scale = 0.8\n","    max_scale = 1.2\n","    sx = np.random.uniform(min_scale, max_scale)\n","    sy = np.random.uniform(min_scale, max_scale)\n","\n","    # --- rotation ---\n","    max_rot_angle = 7\n","    rot_angle = np.random.uniform(-max_rot_angle, max_rot_angle) * np.pi / 180.\n","\n","    # --- shear ---\n","    max_shear_angle = 10\n","    shear_angle = np.random.uniform(-max_shear_angle, max_shear_angle) * np.pi / 180.\n","\n","    # --- translation ---\n","    max_translation = 4\n","    tx = np.random.randint(-max_translation, max_translation)\n","    ty = np.random.randint(-max_translation, max_translation)\n","\n","    tform = AffineTransform(scale=(sx, sy), rotation=rot_angle, shear=shear_angle,\n","                            translation=(tx, ty))\n","    transformed_image = warp(img, tform)\n","    assert transformed_image.ndim == 2\n","    return transformed_image\n","\n","\n","def crop_char_image(image, threshold=5./255.):\n","    \"\"\"\n","    余白消し\n","    \n","    is_black:[0,0,0,0\n","              0,1,1,0\n","              1,0,1,0\n","              1,0,0,0]\n","    is_black_horizontal:[0,\n","                         1,\n","                         1,\n","                         1] np.sum(is_black, axis=1)なのでわかりやすい様に縦に並べた\n","    \n","    left: 1\n","    right: 0\n","    left:height - right: 1:3-0\n","    cropped_image:[0,1,1,0\n","                   1,0,1,0\n","                   1,0,0,0]\n","    余白をむしろ少しんこすほうがいい可能性もある？\n","    \"\"\"\n","    assert image.ndim == 2\n","    is_black = image > threshold #黒かどうか、すなわち文字があることの定義\n","    \n","    is_black_vertical = np.sum(is_black, axis=0) > 0\n","    is_black_horizontal = np.sum(is_black, axis=1) > 0\n","    \n","    left = np.argmax(is_black_horizontal) \n","    right = np.argmax(is_black_horizontal[::-1])\n","    top = np.argmax(is_black_vertical)\n","    bottom = np.argmax(is_black_vertical[::-1])\n","    \n","    height, width = image.shape\n","    cropped_image = image[left:height - right, top:width - bottom]#余白消し\n","    return cropped_image\n","\n","\n","def resize(image, size=(64, 64)):\n","    return cv2.resize(image, size)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qCV40czGvvKf","colab_type":"text"},"source":["## Affine transformation for data augmentation\n","\n","To increase validation score, the number of training data is important. When we can use more number of training data, we can reduce overfitting and validation score becomes high.\n","\n","\"Data augmentation\" is a technic to virtually create extra training data, based on the given training data. For this MNIST task, data augmentation can be achieved by utilizing affine transformation.\n","\n","1. Rotation AffineTransformation\n","2. Translation\n","3. Scale\n","4. Shear"]},{"cell_type":"code","metadata":{"id":"J3HK9OFLvvKg","colab_type":"code","colab":{}},"source":["train_images.shape"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tOtBSpEBvvKl","colab_type":"code","colab":{}},"source":["nrow, ncol = 1, 6\n","\n","fig, axes = plt.subplots(nrow, ncol, figsize=(20, 8))\n","axes = axes.flatten()\n","for i, ax in tqdm(enumerate(axes)):\n","    image, label = train_dataset[0]\n","    ax.imshow(affine_image(image), cmap='Greys')\n","    ax.set_title(f'label: {label}')\n","plt.tight_layout()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0ZrwKE5evvKr","colab_type":"text"},"source":["When the image is slightly rotated, shifted (transformed) or scaled, the image looks like the same label. We can virtually create another image data from one image in such a way."]},{"cell_type":"markdown","metadata":{"id":"9NlY1B42vvKr","colab_type":"text"},"source":["## crop image\n","\n","Here I crop image"]},{"cell_type":"code","metadata":{"id":"UIuVGDi1vvKs","colab_type":"code","colab":{}},"source":["train_dataset.images.shape"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sP7PQS3tvvKv","colab_type":"code","colab":{}},"source":["image.shape"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"urMOES4bvvKx","colab_type":"code","colab":{}},"source":["nrow, ncol = 5, 6\n","\n","fig, axes = plt.subplots(nrow, ncol, figsize=(20, 8))\n","axes = axes.flatten()\n","for i, ax in tqdm(enumerate(axes)):\n","    image, label = train_dataset[i]\n","    ax.imshow(crop_resize(image), cmap='Greys')\n","    ax.set_title(f'label: {label}')\n","plt.tight_layout()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AJpqAu_wvvKz","colab_type":"text"},"source":["## resize image\n","\n","We need to resize image after crop, to align image size for CNN batch training."]},{"cell_type":"code","metadata":{"_kg_hide-input":true,"id":"ieRdQ2XZvvK0","colab_type":"code","colab":{}},"source":["nrow, ncol = 5, 6\n","\n","fig, axes = plt.subplots(nrow, ncol, figsize=(20, 8))\n","axes = axes.flatten()\n","for i, ax in tqdm(enumerate(axes)):\n","    image, label = train_dataset[i]\n","    ax.imshow(resize(crop_char_image(image, threshold=20./255.)), cmap='Greys')\n","    ax.set_title(f'label: {label}')\n","plt.tight_layout()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ysN9JvvNvvK4","colab_type":"text"},"source":["Put everything together with `Transform` class. <br>\n","[Update] I added **albumentations augmentations** introduced in [Bengali: albumentations data augmentation tutorial](https://www.kaggle.com/corochann/bengali-albumentations-data-augmentation-tutorial)."]},{"cell_type":"markdown","metadata":{"id":"XRQcN4uLvvK7","colab_type":"text"},"source":[" # data augmentation Transform クラス"]},{"cell_type":"code","metadata":{"id":"1urcVUtRkb8H","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nDML5p9hvvK9","colab_type":"text"},"source":["この辺りのチューニングは辛そう....学習でチューニングするのは限界あるから　目で見て決めるのかな"]},{"cell_type":"code","metadata":{"_kg_hide-input":false,"id":"O9sBvvTVvvK-","colab_type":"code","colab":{}},"source":["import albumentations as A\n","import numpy as np\n","import albumentations.augmentations.transforms as aat\n","\n","def add_gaussian_noise(x, sigma):\n","    x += np.random.randn(*x.shape) * sigma\n","    x = np.clip(x, 0., 1.)\n","    return x\n","\n","\n","def _evaluate_ratio(ratio):\n","    if ratio <= 0.:\n","        return False\n","    return np.random.uniform() < ratio\n","\n","\n","def apply_aug(aug, image):\n","    return aug(image=image)['image']\n","\n","\n","class Transform:\n","    def __init__(self, affine=True, crop=True, size=(64, 64),\n","                 normalize=True, train=True, threshold=40.,\n","                 sigma=-1., blur_ratio=0., noise_ratio=0., cutout_ratio=0.,\n","                 grid_distortion_ratio=0., elastic_distortion_ratio=0., random_brightness_ratio=0.,\n","                 piece_affine_ratio=0., ssr_ratio=0.):\n","        self.affine = affine\n","        self.crop = crop\n","        self.size = size\n","        self.normalize = normalize\n","        self.train = train\n","        self.threshold = threshold / 255.\n","        self.sigma = sigma / 255.\n","\n","        self.blur_ratio = blur_ratio\n","        self.noise_ratio = noise_ratio\n","        self.cutout_ratio = cutout_ratio\n","        self.grid_distortion_ratio = grid_distortion_ratio\n","        self.elastic_distortion_ratio = elastic_distortion_ratio\n","        self.random_brightness_ratio = random_brightness_ratio\n","        self.piece_affine_ratio = piece_affine_ratio\n","        self.ssr_ratio = ssr_ratio\n","\n","    def __call__(self, example):\n","        if self.train:\n","            x, y = example\n","        else:\n","            x = example\n","        # --- Augmentation ---\n","        if self.affine:\n","            x = affine_image(x)\n","\n","        # --- Train/Test common preprocessing ---\n","        #if self.crop:\n","        #    x = crop_resize(x)\n","        if self.sigma > 0.:\n","            x = add_gaussian_noise(x, sigma=self.sigma)\n","\n","        # albumentations...\n","        x = x.astype(np.float32)\n","        assert x.ndim == 2\n","        # 1. blur\n","        if _evaluate_ratio(self.blur_ratio):\n","            r = np.random.uniform()\n","            if r < 0.25:\n","                x = apply_aug(A.Blur(p=1.0), x)\n","            elif r < 0.5:\n","                x = apply_aug(A.MedianBlur(blur_limit=5, p=1.0), x)\n","            elif r < 0.75:\n","                x = apply_aug(A.GaussianBlur(p=1.0), x)\n","            else:\n","                x = apply_aug(A.MotionBlur(p=1.0), x)\n","\n","        if _evaluate_ratio(self.noise_ratio):\n","            r = np.random.uniform()\n","            if r < 0.50:\n","                x = apply_aug(A.GaussNoise(var_limit=5. / 255., p=1.0), x)\n","            else:\n","                x = apply_aug(A.MultiplicativeNoise(p=1.0), x)\n","\n","        if _evaluate_ratio(self.cutout_ratio):\n","            # A.Cutout(num_holes=2,  max_h_size=2, max_w_size=2, p=1.0)  # Deprecated...\n","            x = apply_aug(aat.CoarseDropout(max_holes=20, max_height=5, max_width=5, p=1.0), x)\n","\n","        if _evaluate_ratio(self.grid_distortion_ratio):\n","            x = apply_aug(A.GridDistortion(p=1.0), x)\n","\n","        if _evaluate_ratio(self.elastic_distortion_ratio):\n","            x = apply_aug(A.ElasticTransform(\n","                sigma=50, alpha=1, alpha_affine=10, p=1.0), x)\n","\n","        if _evaluate_ratio(self.random_brightness_ratio):\n","            # A.RandomBrightness(p=1.0)  # Deprecated...\n","            # A.RandomContrast(p=1.0)    # Deprecated...\n","            x = apply_aug(A.RandomBrightnessContrast(p=1.0), x)\n","\n","        if _evaluate_ratio(self.piece_affine_ratio):\n","            x = apply_aug(A.IAAPiecewiseAffine(p=1.0), x)\n","\n","        if _evaluate_ratio(self.ssr_ratio):\n","            x = apply_aug(A.ShiftScaleRotate(\n","                shift_limit=0.0825,\n","                scale_limit=0.1,\n","                rotate_limit=40,\n","                border_mode=1,\n","                p=1.0), x)\n","\n","        if self.normalize:\n","            x = (x.astype(np.float32) - 0.0692) / 0.2051\n","        if x.ndim == 2:\n","            x = x[None, :, :]\n","        x = x.astype(np.float32)\n","        if self.train:\n","            y = y.astype(np.int64)\n","            return x, y\n","        else:\n","            return x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NIENs8HCvvLC","colab_type":"code","colab":{}},"source":["\n","\n","train_transform = Transform(\n","    size=(image_size, image_size), threshold=20,affine=False, crop=True,\n","    sigma=-1., blur_ratio=0.0, noise_ratio=0.0, cutout_ratio=0.1,\n","    grid_distortion_ratio=0.0, random_brightness_ratio=0.0,\n","    piece_affine_ratio=0.05, ssr_ratio=1.0)\n","# transform = Transform(size=(image_size, image_size)\n","train_dataset = BengaliAIDataset(train_images, train_labels,\n","                                 transform=train_transform)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a5eaHKo7vvLG","colab_type":"text"},"source":["By setting `transform`, its function is called **every time** when we access to the index. Dataset returns different `image` every time! which is useful for training with data augmentation."]},{"cell_type":"code","metadata":{"_kg_hide-input":true,"id":"2Xie30XAvvLM","colab_type":"code","colab":{}},"source":["nrow, ncol = 1, 6\n","\n","fig, axes = plt.subplots(nrow, ncol, figsize=(20, 2))\n","axes = axes.flatten()\n","for i, ax in tqdm(enumerate(axes)):\n","    image, label = train_dataset[1]\n","    ax.imshow(image[0], cmap='Greys')\n","    ax.set_title(f'label: {label}')\n","plt.tight_layout()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XVUgoV-wvvLQ","colab_type":"text"},"source":["Let's final check the processed images, which will be trained by the model."]},{"cell_type":"code","metadata":{"id":"1TJgNRKwvvLR","colab_type":"code","colab":{}},"source":["def rand_bbox(size, lam):\n","    W = size[2]\n","    H = size[3]\n","    cut_rat = np.sqrt(1. - lam)\n","    cut_w = np.int(W * cut_rat)\n","    cut_h = np.int(H * cut_rat)\n","\n","    # uniform\n","    cx = np.random.randint(W)\n","    cy = np.random.randint(H)\n","\n","    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n","    bby1 = np.clip(cy - cut_h // 2, 0, H)\n","    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n","    bby2 = np.clip(cy + cut_h // 2, 0, H)\n","\n","    return bbx1, bby1, bbx2, bby2\n","\n","def cutmix(data, targets1, targets2, targets3, alpha):\n","    indices = torch.randperm(data.size(0))\n","    \n","    shuffled_data = data[indices]\n","    shuffled_targets1 = targets1[indices]\n","    shuffled_targets2 = targets2[indices]\n","    shuffled_targets3 = targets3[indices]\n","\n","    lam = np.random.beta(alpha, alpha)\n","    bbx1, bby1, bbx2, bby2 = rand_bbox(data.size(), lam)\n","    data[:, :, bbx1:bbx2, bby1:bby2] = data[indices, :, bbx1:bbx2, bby1:bby2]\n","    # adjust lambda to exactly match pixel ratio\n","    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (data.size()[-1] * data.size()[-2]))\n","\n","    targets = [targets1, shuffled_targets1, targets2, shuffled_targets2, targets3, shuffled_targets3, lam]\n","    return data, targets\n","\n","def mixup(data, targets1, targets2, targets3, alpha):\n","    indices = torch.randperm(data.size(0))\n","    shuffled_data = data[indices]\n","    shuffled_targets1 = targets1[indices]\n","    shuffled_targets2 = targets2[indices]\n","    shuffled_targets3 = targets3[indices]\n","\n","    lam = np.random.beta(alpha, alpha)\n","    data = data * lam + shuffled_data * (1 - lam)\n","    targets = [targets1, shuffled_targets1, targets2, shuffled_targets2, targets3, shuffled_targets3, lam]\n","\n","    return data, targets"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"_kg_hide-input":true,"id":"FRrsgvukvvLV","colab_type":"code","colab":{}},"source":["nrow, ncol = 5, 6\n","\n","fig, axes = plt.subplots(nrow, ncol, figsize=(20, 8))\n","axes = axes.flatten()\n","for i, ax in tqdm(enumerate(axes)):\n","    image, label = train_dataset[i]\n","    ax.imshow(image[0], cmap='Greys')\n","    ax.set_title(f'label: {label}')\n","plt.tight_layout()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dDPcom6kvvLa","colab_type":"text"},"source":["<a id=\"model\"></a> \n","# pytorch model & define classifier"]},{"cell_type":"code","metadata":{"_kg_hide-input":true,"id":"g_GLDm8vvvLb","colab_type":"code","colab":{}},"source":["import torch\n","\n","#Linear でResnetがやってることをやりたい\n","def residual_add(lhs, rhs):\n","    #lhs:linear のアウトプット,rhs:linearのインプット\n","    lhs_ch, rhs_ch = lhs.shape[1], rhs.shape[1]\n","    if lhs_ch < rhs_ch:#インプットの方が次元でかいなら削って足す\n","        out = lhs + rhs[:, :lhs_ch]\n","    elif lhs_ch > rhs_ch:\n","        #アウトプットの方が次元ならば、アウトプットを削って足して削った部分をくっつける\n","        out = torch.cat([lhs[:, :rhs_ch] + rhs, lhs[:, rhs_ch:]], dim=1)\n","    else:\n","        out = lhs + rhs\n","    return out\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"_kg_hide-input":true,"id":"wJSMq3-cvvLd","colab_type":"code","colab":{}},"source":["#あんまりここは読むいみないと思われる。。。\n","#nn.Linearは先に入力と出力の次元を決めるけど,動的に決めるLazyLinear書いてる \n","#つまりkeras的なことをやりたい\n","from typing import List\n","\n","import torch\n","from torch import nn\n","from torch.nn.parameter import Parameter\n","\n","\n","class LazyLoadModule(nn.Module):\n","    \"\"\"Lazy buffer/parameter loading using load_state_dict_pre_hook\n","\n","    Define all buffer/parameter in `_lazy_buffer_keys`/`_lazy_parameter_keys` and\n","    save buffer with `register_buffer`/`register_parameter`\n","    method, which can be outside of __init__ method.\n","    Then this module can load any shape of Tensor during de-serializing.\n","\n","    Note that default value of lazy buffer is torch.Tensor([]), while lazy parameter is None.\n","    \"\"\"\n","    _lazy_buffer_keys: List[str] = []     # It needs to be override to register lazy buffer\n","    _lazy_parameter_keys: List[str] = []  # It needs to be override to register lazy parameter\n","\n","    def __init__(self):\n","        super(LazyLoadModule, self).__init__()\n","        for k in self._lazy_buffer_keys:\n","            self.register_buffer(k, torch.tensor([]))\n","        for k in self._lazy_parameter_keys:\n","            self.register_parameter(k, None)\n","        self._register_load_state_dict_pre_hook(self._hook)\n","\n","    def _hook(self, state_dict, prefix, local_metadata, strict, missing_keys,\n","             unexpected_keys, error_msgs):\n","        for key in self._lazy_buffer_keys:\n","            self.register_buffer(key, state_dict[prefix + key])\n","\n","        for key in self._lazy_parameter_keys:\n","            self.register_parameter(key, Parameter(state_dict[prefix + key]))\n","\n","            \n","            \n","            \n","            \n","import math\n","import torch\n","from torch.nn import init\n","from torch.nn.parameter import Parameter\n","import torch.nn.functional as F\n","\n","\n","class LazyLinear(LazyLoadModule):\n","    \"\"\"Linear module with lazy input inference\n","\n","    `in_features` can be `None`, and it is determined at the first time of forward step dynamically.\n","    \"\"\"\n","\n","    __constants__ = ['bias', 'in_features', 'out_features']\n","    _lazy_parameter_keys = ['weight']\n","\n","    def __init__(self, in_features, out_features, bias=True):\n","        super(LazyLinear, self).__init__()\n","        self.in_features = in_features\n","        self.out_features = out_features\n","        if bias:\n","            self.bias = Parameter(torch.Tensor(out_features))\n","        else:\n","            self.register_parameter('bias', None)\n","\n","        if in_features is not None:\n","            self.weight = Parameter(torch.Tensor(out_features, in_features))\n","            self.reset_parameters()\n","\n","    def reset_parameters(self):\n","        init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n","        if self.bias is not None:\n","            fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n","            bound = 1 / math.sqrt(fan_in)\n","            init.uniform_(self.bias, -bound, bound)\n","\n","    def forward(self, input):\n","        if self.weight is None:\n","            self.in_features = input.shape[-1]\n","            self.weight = Parameter(torch.Tensor(self.out_features, self.in_features))\n","            self.reset_parameters()\n","\n","            # Need to send lazy defined parameter to device...\n","            self.to(input.device)\n","        return F.linear(input, self.weight, self.bias)\n","\n","    def extra_repr(self):\n","        return 'in_features={}, out_features={}, bias={}'.format(\n","            self.in_features, self.out_features, self.bias is not None\n","        )\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"_kg_hide-input":true,"id":"VV0hKuH4vvLf","colab_type":"code","colab":{}},"source":["from torch import nn\n","import torch.nn.functional as F\n","import math\n","\n","class LinearBlock(nn.Module):\n","    \"\"\"\n","    linear→BN→activation→residual_add→dropout\n","    \"\"\"\n","    def __init__(self, in_features, out_features, bias=True,\n","                 use_bn=True, activation=F.relu, dropout_ratio=-1, residual=False,):\n","        super(LinearBlock, self).__init__()\n","        if in_features is None:\n","            self.linear = LazyLinear(in_features, out_features, bias=bias)\n","        else:\n","            self.linear = nn.Linear(in_features, out_features, bias=bias)\n","        if use_bn:\n","            self.bn = nn.BatchNorm1d(out_features)\n","        if dropout_ratio > 0.:\n","            self.dropout = nn.Dropout(p=dropout_ratio)\n","        else:\n","            self.dropout = None\n","        self.activation = activation\n","        self.use_bn = use_bn\n","        self.dropout_ratio = dropout_ratio\n","        self.residual = residual\n","\n","    def __call__(self, x):\n","        h = self.linear(x)\n","        if self.use_bn:\n","            h = self.bn(h)\n","        if self.activation is not None:\n","            h = self.activation(h)\n","        if self.residual:\n","            h = residual_add(h, x)\n","        if self.dropout_ratio > 0:\n","            h = self.dropout(h)\n","        return h"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VxhWBIVYvvLi","colab_type":"code","colab":{}},"source":["# pretrained='imagenet'\n","# # model_name='se_resnext50_32x4d'\n","# !pip install ./pretrainedmodels-0.7.4\\ 2/pretrainedmodels-0.7.4/ > /dev/null # no output\n","\n","# import pretrainedmodels"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jf18uTvfvvLu","colab_type":"code","colab":{}},"source":["from torch.nn.parameter import Parameter\n","def gem(x, p=3, eps=1e-6):\n","    return F.avg_pool2d(x.clamp(min=eps).pow(p), (x.size(-2), x.size(-1))).pow(1./p)\n","class GeM(nn.Module):\n","    def __init__(self, p=3, eps=1e-6):\n","        super(GeM,self).__init__()\n","        self.p = Parameter(torch.ones(1)*p)\n","        self.eps = eps\n","    def forward(self, x):\n","        return gem(x, p=self.p, eps=self.eps)       \n","    def __repr__(self):\n","        return self.__class__.__name__ + '(' + 'p=' + '{:.4f}'.format(self.p.data.tolist()[0]) + ', ' + 'eps=' + str(self.eps) + ')'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"isVaCT5gvvLv","colab_type":"code","colab":{}},"source":["# import os \n","# os.chdir(\"/home/chikazoe/operation/bengali/\")\n","!pip install pretrainedmodels"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BDHlPyHyvvLz","colab_type":"code","colab":{}},"source":["import pretrainedmodels\n","\n","# model = pretrainedmodels.__dict__[model_name](pretrained=None)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ah9IIRevvvL7","colab_type":"code","colab":{}},"source":["# gemm = GeM()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9fchjE97vvL_","colab_type":"code","colab":{}},"source":["@torch.jit.script\n","def mish(input):\n","    '''\n","    Applies the mish function element-wise:\n","    mish(x) = x * tanh(softplus(x)) = x * tanh(ln(1 + exp(x)))\n","    See additional documentation for mish class.\n","    '''\n","    return input * torch.tanh(F.softplus(input))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JNZww-odvvMD","colab_type":"code","colab":{}},"source":["import pretrainedmodels\n","import torch\n","from torch import nn\n","import torch.nn.functional as F\n","from torch.nn import Sequential\n","\n","\n","class PretrainedCNN(nn.Module):\n","    def __init__(self, model_name='se_resnext101_32x4d',\n","                 in_channels=1, out_dim=10, use_bn=True,\n","                 pretrained='imagenet'):\n","        super(PretrainedCNN, self).__init__()\n","        self.conv0 = nn.Conv2d(\n","            in_channels, 3, kernel_size=3, stride=1, padding=1, bias=True)\n","        self.convgra = nn.Conv2d(\n","            2048, 2048, kernel_size=3, stride=1, padding=1, bias=False)\n","        self.convvo = nn.Conv2d(\n","            2048, 2048, kernel_size=3, stride=1, padding=1, bias=False)\n","        self.convco = nn.Conv2d(\n","            2048, 2048, kernel_size=3, stride=1, padding=1, bias=False)\n","        \n","        self.batchNgra = nn.BatchNorm2d(2048)\n","        self.batchNvo = nn.BatchNorm2d(2048)\n","        self.batchNco = nn.BatchNorm2d(2048)\n","        self.base_model = pretrainedmodels.__dict__[model_name](pretrained=pretrained)\n","        activation = mish\n","        self.gemgra = GeM()\n","        self.gemvo = GeM()\n","        self.gemco = GeM()\n","        self.do_pooling = False\n","        if self.do_pooling:\n","            inch = self.base_model.last_linear.in_features\n","        else:\n","            inch = None\n","        hdim = 1024\n","        self.lin1 = LinearBlock(inch, hdim, use_bn=use_bn, activation=activation, residual=False)\n","        self.lin2 = LinearBlock(hdim, 168, use_bn=use_bn, activation=None, residual=False)\n","        self.lin3 = LinearBlock(inch, hdim, use_bn=use_bn, activation=activation, residual=False)\n","        self.lin4 = LinearBlock(hdim, 11, use_bn=use_bn, activation=None, residual=False)\n","        self.lin5 = LinearBlock(inch, hdim, use_bn=use_bn, activation=activation, residual=False)\n","        self.lin6 = LinearBlock(hdim, 7, use_bn=use_bn, activation=None, residual=False)\n","        #self.lin_layers = Sequential(lin1, lin2)\n","\n","    def forward(self, x):\n","        h = self.conv0(x)#self.base_modelはchannelが３を想定しているので1から３に変換\n","        h = self.base_model.features(h) #imagenetの最後の分類部分(linear)まででなく上層のCNNの部分の結果を取り出す\n","        h = mish(h)\n","        grah = self.gemgra(self.batchNgra(self.convgra(h)))\n","        voh = self.gemvo(self.batchNvo(self.convvo(h)))\n","        coh = self.gemco(self.batchNco(self.convco(h)))\n","        \n","        if self.do_pooling:#謎 平均ではなく足す\n","            h = torch.sum(h, dim=(-1, -2))\n","        else:\n","            # [128, 2048, 4, 4] when input is (128, 128)\n","            # [128, 2048, 2, 2] when input is (64, 64)\n","            bs, ch, height, width = grah.shape\n","            \n","            grah = grah.view(bs, ch*height*width)\n","            voh = voh.view(bs, ch*height*width)\n","            coh = coh.view(bs, ch*height*width)\n","        features1 = self.lin1(grah)\n","        grapheme = self.lin2(features1)\n","        features2 = self.lin3(voh)\n","        vowel = self.lin4(features2)\n","        features3 = self.lin5(coh)\n","        consonant = self.lin6(features3)\n","        return features1,grapheme,features2,vowel,features3,consonant"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rBEveVYOvvMR","colab_type":"text"},"source":["## Classifier"]},{"cell_type":"code","metadata":{"id":"dXctQwCcvvMS","colab_type":"code","colab":{}},"source":["def cutmix_criterion(features,preds1,preds2,preds3, targets,center_loss_grapheme,center_loss_vowel,center_loss_consonant):\n","    targets1, targets2,targets3, targets4,targets5, targets6, lam = targets[0], targets[1], targets[2], targets[3], targets[4], targets[5], targets[6]\n","    criterion = nn.CrossEntropyLoss(reduction='mean')\n","    features1,features2,features3 = features[0],features[1],features[2]\n","    #criterion = FocalLoss(gamma=2,reduced_threshold=0.5)\n","    \n","    loss_grapheme = lam *(criterion(preds1, targets1)+center_loss_grapheme(features1, targets1) * 0.0005)\n","    +(1 - lam) * (criterion(preds1, targets2)+center_loss_grapheme(features1, targets2)* 0.0005)\n","    loss_vowel = lam * (criterion(preds2, targets3)+center_loss_vowel(features2, targets3) * 0.0005)\n","    + (1 - lam) * (criterion(preds2, targets4)+center_loss_vowel(features2, targets4) * 0.0005)\n","    loss_consonant = lam * (criterion(preds3, targets5)+center_loss_consonant(features3, targets5) * 0.0005)\n","    + (1 - lam) * (criterion(preds3, targets6)+center_loss_consonant(features3, targets6) * 0.0005)\n","    loss = loss_grapheme*0.5 + loss_vowel*0.25 + loss_consonant*0.25\n","    return loss,loss_grapheme,loss_vowel,loss_consonant"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"_kg_hide-input":true,"id":"OsyNc0rtvvMX","colab_type":"code","colab":{}},"source":["import torch\n","from torch import nn\n","import torch.nn.functional as F\n","from tqdm import tqdm\n","\n","\n","def accuracy(y, t):\n","    pred_label = torch.argmax(y, dim=1)\n","    count = pred_label.shape[0]\n","    correct = (pred_label == t).sum().type(torch.float32)\n","    acc = correct / count\n","    return acc\n","\n","\n","class BengaliClassifier(nn.Module):\n","    def __init__(self, predictor,metric_loss, n_grapheme=168, n_vowel=11, n_consonant=7):\n","        super(BengaliClassifier, self).__init__()\n","        self.n_grapheme = n_grapheme\n","        self.n_vowel = n_vowel\n","        self.n_consonant = n_consonant\n","        self.n_total_class = self.n_grapheme + self.n_vowel + self.n_consonant\n","        self.predictor = predictor \n","        self.metrics_keys = [\n","            'loss', 'loss_grapheme', 'loss_vowel', 'loss_consonant',\n","            'acc_grapheme', 'acc_vowel', 'acc_consonant']\n","\n","        \n","        self.center_loss_grapheme,self.center_loss_vowel,self.center_loss_consonant = metric_loss\n","    def forward(self, x, y=None,iscutmix=False):\n","        features1,grapheme,features2,vowel,features3,consonant = self.predictor(x)\n","        features = [features1,features2,features3]\n","        #if isinstance(pred, tuple):\n","        #    assert len(pred) == 3\n","        #    preds = pred\n","#\n","#        else:\n","#            assert pred.shape[1] == self.n_total_class\n","#            preds = torch.split(pred, [self.n_grapheme, self.n_vowel, self.n_consonant], dim=1)\n","        if iscutmix:\n","            loss,loss_grapheme,loss_vowel,loss_consonant = cutmix_criterion(features,grapheme,vowel,consonant,y,\n","                                                                           self.center_loss_grapheme,\n","                                                                           self.center_loss_vowel,\n","                                                                           self.center_loss_consonant)\n","            metrics = {\n","                'loss': loss.item(),\n","                'loss_grapheme': loss_grapheme.item(),\n","                'loss_vowel': loss_vowel.item(),\n","                'loss_consonant': loss_consonant.item(),\n","                'acc_grapheme': 0,\n","                'acc_vowel': 0,\n","                'acc_consonant': 0,\n","            }\n","        else:\n","            loss_grapheme = F.cross_entropy(grapheme, y[:, 0]) + self.center_loss_grapheme(features1, y[:, 0]) * 0.0005\n","            loss_vowel = F.cross_entropy(vowel, y[:, 1]) + self.center_loss_vowel(features2, y[:, 1]) * 0.0005\n","            loss_consonant = F.cross_entropy(consonant, y[:, 2]) + self.center_loss_consonant(features3, y[:, 2]) * 0.0005\n","            loss = loss_grapheme*0.5 + loss_vowel*0.25 + loss_consonant*0.25\n","            metrics = {\n","                'loss': loss.item(),\n","                'loss_grapheme': loss_grapheme.item(),\n","                'loss_vowel': loss_vowel.item(),\n","                'loss_consonant': loss_consonant.item(),\n","                'acc_grapheme': accuracy(grapheme, y[:, 0]),\n","                'acc_vowel': accuracy(vowel, y[:, 1]),\n","                'acc_consonant': accuracy(consonant, y[:, 2]),\n","            }\n","        return loss, metrics, torch.cat([grapheme,vowel,consonant],dim=1)\n","\n","    def calc(self, data_loader):\n","        device: torch.device = next(self.parameters()).device\n","        self.eval()\n","        output_list = []\n","        with torch.no_grad():\n","            for batch in tqdm(data_loader):\n","                # TODO: support general preprocessing.\n","                # If `data` is not `Data` instance, `to` method is not supported!\n","                batch = batch.to(device)\n","                pred = self.predictor(batch)\n","                output_list.append(pred)\n","        output = torch.cat(output_list, dim=0)\n","        preds = torch.split(output, [self.n_grapheme, self.n_vowel, self.n_consonant], dim=1)\n","        return preds\n","\n","    def predict_proba(self, data_loader):\n","        preds = self.calc(data_loader)\n","        return [F.softmax(p, dim=1) for p in preds]\n","\n","    def predict(self, data_loader):\n","        preds = self.calc(data_loader)\n","        pred_labels = [torch.argmax(p, dim=1) for p in preds]\n","        return pred_labels\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"a2k83IvVvvMa","colab_type":"code","colab":{}},"source":["torch.split(torch.rand(30,186),[168, 7, 11], dim=1)[0]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-fdxOpPQvvMc","colab_type":"code","colab":{}},"source":["torch.cat([torch.rand(30,186),torch.rand(30,186)],dim=1).shape"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7kiFJPCJvvMd","colab_type":"text"},"source":["<a id=\"train\"></a>\n","# Training code"]},{"cell_type":"code","metadata":{"id":"sNkPUm4_vvMe","colab_type":"code","colab":{}},"source":["# train_fold = pd.read_csv(\"./train_with_fold.csv\")\n","train_labels.shape\n","def myfunc(num):\n","    return str(num).zfill(3)\n","vfunc=np.vectorize(myfunc)\n","c0 = vfunc(train_labels[:,0]).astype(object)\n","c1 = vfunc(train_labels[:,1]).astype(object)\n","c2 = vfunc(train_labels[:,2]).astype(object)\n","cat_label = c0 + c1 + c2\n","print(cat_label[:5])\n","print(len(cat_label))\n","from sklearn.model_selection import StratifiedKFold\n","\n","\n","skf = StratifiedKFold(n_splits=n_splits, random_state=seed, shuffle=True)\n","i = 0\n","for train_index, test_index in skf.split(np.ones(len(cat_label)), cat_label):\n","    train_idx = train_index\n","    valid_idx = test_index\n","    if i == 0:\n","        break\n","    i += 1\n","print(len(train_idx))\n","print(train_idx[:5])\n","print(len(valid_idx))\n","print(valid_idx[:5])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_--8yJ2KvvMi","colab_type":"text"},"source":["## prepare data"]},{"cell_type":"code","metadata":{"_kg_hide-input":true,"id":"aaG-Z5pKvvMk","colab_type":"code","outputId":"c625d02f-310e-448c-91a6-a7eb08263e5a","executionInfo":{"status":"ok","timestamp":1583986144749,"user_tz":-540,"elapsed":21431,"user":{"displayName":"K Rie","photoUrl":"","userId":"01922819648522133437"}},"colab":{"base_uri":"https://localhost:8080/"}},"source":["#n_dataset = len(train_images)\n","#train_data_size = 200 if debug else int(n_dataset * 0.9)\n","#valid_data_size = 100 if debug else int(n_dataset - train_data_size)\n","\n","#perm = np.random.RandomState(777).permutation(n_dataset)\n","#print('perm', perm)\n","train_transform = Transform(\n","    size=(image_size, image_size), threshold=20,\n","    sigma=-1., blur_ratio=0.0, noise_ratio=0.0, cutout_ratio=0.0,\n","    grid_distortion_ratio=0.0, random_brightness_ratio=0.0,\n","    piece_affine_ratio=0.0, ssr_ratio=0.0)\n","\n","valid_trainsform = Transform(affine=False, crop=True, size=(image_size, image_size))\n","\n","train_dataset = BengaliAIDataset(\n","    train_images[train_idx], train_labels[train_idx], transform=train_transform)\n","valid_dataset = BengaliAIDataset(\n","    train_images[valid_idx], train_labels[valid_idx], transform=valid_trainsform)\n","print('train_dataset', len(train_dataset), 'valid_dataset', len(valid_dataset))\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["train_dataset 160672 valid_dataset 40168\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"J849ApZUvvMl","colab_type":"code","colab":{}},"source":["center_loss_grapheme = CenterLoss(num_classes=168, feat_dim=1024, use_gpu=True)\n","center_loss_vowel = CenterLoss(num_classes=11, feat_dim=1024, use_gpu=True)\n","center_loss_consonant = CenterLoss(num_classes=7, feat_dim=1024, use_gpu=True)\n","metric_loss = (center_loss_grapheme,center_loss_vowel,center_loss_consonant)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"u9hVkyeHvvMn","colab_type":"code","outputId":"01ee90bc-e9d0-4eff-e855-e19d95d9f079","executionInfo":{"status":"ok","timestamp":1583986145085,"user_tz":-540,"elapsed":9194,"user":{"displayName":"K Rie","photoUrl":"","userId":"01922819648522133437"}},"colab":{"base_uri":"https://localhost:8080/"}},"source":["# --- Model ---\n","device = torch.device(device)\n","n_grapheme = 168\n","n_vowel = 11\n","n_consonant = 7\n","n_total = n_grapheme + n_vowel + n_consonant\n","print('n_total', n_total)\n","# Set pretrained='imagenet' to download imagenet pretrained model...\n","print(\"model_name:\",model_name,\"pretrained:\",pretrained)\n","predictor = PretrainedCNN(in_channels=1, out_dim=n_total, model_name=model_name, pretrained=None)\n","if load_model_path:\n","    predictor.load_state_dict(torch.load(load_model_path))\n","print('predictor', type(predictor))\n","\n","classifier = BengaliClassifier(predictor,metric_loss).to(device)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["n_total 186\n","model_name: se_resnext50_32x4d pretrained: imagenet\n","predictor <class '__main__.PretrainedCNN'>\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"QJ-YRue4vvMp","colab_type":"text"},"source":["## Ignite utility\n","\n","pytorch-ignite utility class for training"]},{"cell_type":"markdown","metadata":{"id":"5dLFUMtavvMq","colab_type":"text"},"source":["### igniteはpytorchのラッパー\n","ログ周りとかモデル保存周りをきれいに書けるらしい\n","igniteあたりのコメントは多分どっか間違ってる"]},{"cell_type":"markdown","metadata":{"id":"QGipzD5ivvMq","colab_type":"text"},"source":["### https://fam-taro.hatenablog.com/entry/2018/12/25/021346"]},{"cell_type":"code","metadata":{"_kg_hide-input":true,"id":"fNC63xsyvvMr","colab_type":"code","colab":{}},"source":["import json\n","from logging import getLogger\n","import numpy\n","\n","def save_json(filepath, params):\n","    with open(filepath, 'w') as f:\n","        json.dump(params, f, indent=4)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GcDY7wzPvvMt","colab_type":"code","outputId":"83dce627-0ad1-461e-c683-2f297e1b44bc","executionInfo":{"status":"ok","timestamp":1583986145090,"user_tz":-540,"elapsed":8593,"user":{"displayName":"K Rie","photoUrl":"","userId":"01922819648522133437"}},"colab":{"base_uri":"https://localhost:8080/"}},"source":["!pip install pytorch-ignite"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting pytorch-ignite\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/35/55/41e8a995876fd2ade29bdba0c3efefa38e7d605cb353c70f3173c04928b5/pytorch_ignite-0.3.0-py2.py3-none-any.whl (103kB)\n","\r\u001b[K     |███▏                            | 10kB 22.4MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 20kB 1.8MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 30kB 2.3MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 40kB 1.7MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 51kB 1.9MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 61kB 2.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 71kB 2.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 81kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 92kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 102kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 112kB 2.9MB/s \n","\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from pytorch-ignite) (1.4.0)\n","Installing collected packages: pytorch-ignite\n","Successfully installed pytorch-ignite-0.3.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"_kg_hide-input":true,"id":"q8waE07jvvMw","colab_type":"code","colab":{}},"source":["import os\n","from logging import getLogger\n","from time import perf_counter\n","\n","import pandas as pd\n","import torch\n","# from chainer_chemistry.utils import save_json\n","\n","from ignite.engine.engine import Engine, Events\n","from ignite.metrics import Average\n","\n","\n","class DictOutputTransform:\n","    \"\"\"\n","    関数update_fnのreturnから欲しい損失や評価値を撮ってくるための関数\n","    \"\"\"\n","    def __init__(self, key, index=0):\n","        self.key = key\n","        self.index = index\n","\n","    def __call__(self, x):\n","        if self.index >= 0:\n","            x = x[self.index]\n","        return x[self.key]\n","\n","def cutmix_or_mixup_train(images,labels):\n","    if np.random.rand()<0.2:\n","        label1, label2, label3 = labels[:,0],labels[:,1],labels[:,2]\n","        images, targets = cutmix(images, label1, label2, label3, 1)\n","        return images, targets,True\n","    elif np.random.rand()<1.1:\n","        label1, label2, label3 = labels[:,0],labels[:,1],labels[:,2]\n","        images, targets = mixup(images, label1, label2, label3, 1)\n","        return images, targets,True\n","    else:\n","        return images,labels,False\n","\n","    \n","\n","def create_trainer(classifier, optimizer,opt_cen_gr,opt_cen_co,opt_cen_vo, device):\n","    \n","    \"\"\"\n","    classifier.metrics_keys: \n","    ['loss', 'loss_grapheme', 'loss_vowel', 'loss_consonant',\n","            'acc_grapheme', 'acc_vowel', 'acc_consonant']\n","    \n","    \"\"\"\n","    classifier.to(device)\n","\n","    def update_fn(engine, batch):\n","        classifier.train()\n","        optimizer.zero_grad()\n","        opt_cen_gr.zero_grad()\n","        opt_cen_co.zero_grad()\n","        opt_cen_vo.zero_grad()\n","        # batch = [elem.to(device) for elem in batch]\n","        x, y = [elem.to(device) for elem in batch]\n","        \n","        x,y1,iscutmix = cutmix_or_mixup_train(x,y)\n","        loss, metrics, pred_y = classifier(x, y1,iscutmix)\n","        \n","        loss.backward()\n","        optimizer.step()\n","        for param,param2,param3 in zip(center_loss_grapheme.parameters(),center_loss_vowel.parameters(),center_loss_consonant.parameters()):\n","            param.grad.data *= (1./0.0005)\n","            param2.grad.data *= (1./0.0005)\n","            param3.grad.data *= (1./0.0005)\n","        opt_cen_gr.step()\n","        opt_cen_co.step()\n","        opt_cen_vo.step()\n","        return metrics, pred_y, y\n","    trainer = Engine(update_fn)# Engine()でtraine.run(loader,epoch)で学習ができる様になる\n","                                 #Engine(学習関数:update_fn)する学習関数はengineとバッチを引数にする\n","        \n","\n","    for key in classifier.metrics_keys:\n","        Average(output_transform=DictOutputTransform(key)).attach(trainer, key)\n","        #Averageはigniteの平均計算を計算する関数\n","        #Average().attach(trainer,name)でtrainer(ここではupdate_fn)のアウトプットにnameで名前をつけて平均の監視をする\n","        #DictOutputTransform(key)によって'loss'や'loss_grapheme'の値を取ってきている\n","    return trainer\n","\n","\n","def create_evaluator(classifier, device):\n","    classifier = classifier.to(device)\n","\n","    def update_fn(engine, batch):\n","        classifier.eval()\n","        with torch.no_grad():\n","            # batch = [elem.to(device) for elem in batch]\n","            x, y = [elem.to(device) for elem in batch]\n","            _, metrics, pred_y = classifier(x, y,iscutmix=False)\n","            return metrics, pred_y, y\n","    evaluator = Engine(update_fn)\n","\n","    for key in classifier.metrics_keys:\n","        Average(output_transform=DictOutputTransform(key)).attach(evaluator, key)\n","    return evaluator\n","\n","\n","class LogReport:\n","    \"\"\"\n","    'epoch','iteration'metrics\n","    \n","    \"\"\"\n","    def __init__(self, evaluator=None, dirpath=None, logger=None):\n","        self.evaluator = evaluator\n","        self.dirpath = str(dirpath) if dirpath is not None else None\n","        self.logger = logger or getLogger(__name__)\n","\n","        self.reported_dict = {}  # To handle additional parameter to monitor\n","        self.history = []\n","        self.start_time = perf_counter()\n","\n","    def report(self, key, value):\n","        #自分で追加してLogを取れる\n","        self.reported_dict[key] = value\n","\n","    def __call__(self, engine):\n","        elapsed_time = perf_counter() - self.start_time\n","        elem = {'epoch': engine.state.epoch,\n","                'iteration': engine.state.iteration}\n","        elem.update({f'train/{key}': value\n","                     for key, value in engine.state.metrics.items()})\n","        if self.evaluator is not None:\n","            elem.update({f'valid/{key}': value\n","                         for key, value in self.evaluator.state.metrics.items()})\n","        elem.update(self.reported_dict)\n","        elem['elapsed_time'] = elapsed_time\n","        self.history.append(elem)\n","        if self.dirpath:\n","            save_json(os.path.join(self.dirpath, 'logcentercutout.json'), self.history)\n","            self.get_dataframe().to_csv(os.path.join(self.dirpath, 'logcentercutout.csv'), index=False)\n","\n","        # --- print ---\n","        msg = ''\n","        for key, value in elem.items():\n","            if key in ['iteration']:\n","                # skip printing some parameters...\n","                continue\n","            elif isinstance(value, int):\n","                msg += f'{key} {value: >6d} '\n","            else:\n","                msg += f'{key} {value: 8f} '\n","#         self.logger.warning(msg)\n","        print(msg)\n","\n","        # --- Reset ---\n","        self.reported_dict = {}\n","\n","    def get_dataframe(self):\n","        df = pd.DataFrame(self.history)\n","        return df\n","\n","\n","class SpeedCheckHandler:\n","    \"\"\"\n","    iter/secを計算する\n","    \"\"\"\n","    def __init__(self, iteration_interval=10, logger=None):\n","        self.iteration_interval = iteration_interval\n","        self.logger = logger or getLogger(__name__)\n","        self.prev_time = perf_counter()\n","\n","    def __call__(self, engine: Engine):\n","        if engine.state.iteration % self.iteration_interval == 0:\n","            cur_time = perf_counter()\n","            spd = self.iteration_interval / (cur_time - self.prev_time)\n","            self.logger.warning(f'{spd} iter/sec')\n","            # reset\n","            self.prev_time = cur_time\n","\n","    def attach(self, engine: Engine):\n","        engine.add_event_handler(Events.ITERATION_COMPLETED, self)\n","\n","\n","class ModelSnapshotHandler:\n","    \"\"\"\n","    モデルの保存\n","    \"\"\"\n","    def __init__(self, model, filepath='model_{count:06}.pt',\n","                 interval=1, logger=None):\n","        self.model = model\n","        self.filepath: str = str(filepath)\n","        self.interval = interval\n","        self.logger = logger or getLogger(__name__)\n","        self.count = 0\n","\n","    def __call__(self, engine: Engine):\n","        self.count += 1\n","        if self.count % self.interval == 0:\n","            filepath = self.filepath.format(count=self.count)\n","            torch.save(self.model.state_dict(), filepath)\n","            #self.logger.warning(f'save model to {filepath}...')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"_kg_hide-input":true,"id":"HCyslEhVvvM3","colab_type":"code","colab":{}},"source":["import warnings\n","\n","import torch\n","\n","from ignite.metrics.metric import Metric\n","\n","\n","class EpochMetric(Metric):\n","    \"\"\"Class for metrics that should be computed on the entire output history of a model.\n","    Model's output and targets are restricted to be of shape `(batch_size, n_classes)`. Output\n","    datatype should be `float32`. Target datatype should be `long`.\n","\n","    .. warning::\n","\n","        Current implementation stores all input data (output and target) in as tensors before computing a metric.\n","        This can potentially lead to a memory error if the input data is larger than available RAM.\n","\n","\n","    - `update` must receive output of the form `(y_pred, y)`.\n","\n","    If target shape is `(batch_size, n_classes)` and `n_classes > 1` than it should be binary: e.g. `[[0, 1, 0, 1], ]`.\n","\n","    Args:\n","        compute_fn (callable): a callable with the signature (`torch.tensor`, `torch.tensor`) takes as the input\n","            `predictions` and `targets` and returns a scalar.\n","        output_transform (callable, optional): a callable that is used to transform the\n","            :class:`~ignite.engine.Engine`'s `process_function`'s output into the\n","            form expected by the metric. This can be useful if, for example, you have a multi-output model and\n","            you want to compute the metric with respect to one of the outputs.\n","\n","    \"\"\"\n","\n","    def __init__(self, compute_fn, output_transform=lambda x: x):\n","\n","        if not callable(compute_fn):\n","            raise TypeError(\"Argument compute_fn should be callable.\")\n","\n","        super(EpochMetric, self).__init__(output_transform=output_transform)\n","        self.compute_fn = compute_fn\n","\n","    def reset(self):\n","        self._predictions = torch.tensor([], dtype=torch.float32)\n","        self._targets = torch.tensor([], dtype=torch.long)\n","\n","    def update(self, output):\n","        y_pred, y = output\n","        self._predictions = torch.cat([self._predictions, y_pred], dim=0)\n","        self._targets = torch.cat([self._targets, y], dim=0)\n","\n","        # Check once the signature and execution of compute_fn\n","        if self._predictions.shape == y_pred.shape:\n","            try:\n","                self.compute_fn(self._predictions, self._targets)\n","            except Exception as e:\n","                warnings.warn(\"Probably, there can be a problem with `compute_fn`:\\n {}.\".format(e),\n","                              RuntimeWarning)\n","\n","    def compute(self):\n","        return self.compute_fn(self._predictions, self._targets)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"_kg_hide-input":true,"id":"O4d9vYmWvvM5","colab_type":"code","colab":{}},"source":["import numpy as np\n","import sklearn.metrics\n","import torch\n","\n","\n","def macro_recall(pred_y, y, n_grapheme=168, n_vowel=11, n_consonant=7):\n","    \n","    pred_y = torch.split(pred_y, [n_grapheme, n_vowel, n_consonant], dim=1)\n","    pred_labels = [torch.argmax(py, dim=1).cpu().numpy() for py in pred_y]\n","\n","    y = y.cpu().numpy()\n","    # pred_y = [p.cpu().numpy() for p in pred_y]\n","\n","    recall_grapheme = sklearn.metrics.recall_score(pred_labels[0], y[:, 0], average='macro')\n","    recall_vowel = sklearn.metrics.recall_score(pred_labels[1], y[:, 1], average='macro')\n","    recall_consonant = sklearn.metrics.recall_score(pred_labels[2], y[:, 2], average='macro')\n","    scores = [recall_grapheme, recall_vowel, recall_consonant]\n","    final_score = np.average(scores, weights=[2, 1, 1])\n","    # print(f'recall: grapheme {recall_grapheme}, vowel {recall_vowel}, consonant {recall_consonant}, '\n","    #       f'total {final_score}, y {y.shape}')\n","    return final_score\n","\n","\n","def calc_macro_recall(solution, submission):\n","    # solution df, submission df\n","    scores = []\n","    for component in ['grapheme_root', 'consonant_diacritic', 'vowel_diacritic']:\n","        y_true_subset = solution[solution[component] == component]['target'].values\n","        y_pred_subset = submission[submission[component] == component]['target'].values\n","        scores.append(sklearn.metrics.recall_score(\n","            y_true_subset, y_pred_subset, average='macro'))\n","    final_score = np.average(scores, weights=[2, 1, 1])\n","    return final_score\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"_kg_hide-output":true,"scrolled":true,"id":"-iNUERq8vvM7","colab_type":"code","outputId":"2fae4299-6c13-4fcb-945f-83145cc01b85","executionInfo":{"status":"ok","timestamp":1583986145095,"user_tz":-540,"elapsed":5378,"user":{"displayName":"K Rie","photoUrl":"","userId":"01922819648522133437"}},"colab":{"base_uri":"https://localhost:8080/"}},"source":["import argparse\n","from distutils.util import strtobool\n","import os\n","\n","import torch\n","from ignite.contrib.handlers import ProgressBar\n","from ignite.engine import Events\n","from numpy.random.mtrand import RandomState\n","from torch.utils.data.dataloader import DataLoader\n","\n","# --- Training setting ---\n","\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n","\n","\n","optimizer = torch.optim.Adam(classifier.predictor.parameters(), lr=0.001)\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n","    optimizer, mode='min', factor=0.2, patience=3, min_lr=1e-7)\n","\n","optimizer_centloss_grapheme = torch.optim.Adam(classifier.center_loss_grapheme.parameters(), lr=0.1)\n","optimizer_centloss_vowel = torch.optim.Adam(classifier.center_loss_vowel.parameters(), lr=0.1)\n","optimizer_centloss_consonant = torch.optim.Adam(classifier.center_loss_consonant.parameters(), lr=0.1)\n","\n","scheduler_centloss_grapheme = torch.optim.lr_scheduler.ReduceLROnPlateau(\n","    optimizer_centloss_grapheme, mode='min', factor=0.7, patience=3, min_lr=1e-10)\n","\n","scheduler_centloss_vowel = torch.optim.lr_scheduler.ReduceLROnPlateau(\n","    optimizer_centloss_vowel, mode='min', factor=0.7, patience=3, min_lr=1e-10)\n","\n","scheduler_centloss_consonant = torch.optim.lr_scheduler.ReduceLROnPlateau(\n","    optimizer_centloss_consonant, mode='min', factor=0.7, patience=3, min_lr=1e-10)\n","\n","trainer = create_trainer(classifier,\n","                         optimizer,\n","                         optimizer_centloss_grapheme,\n","                         optimizer_centloss_vowel,\n","                         optimizer_centloss_consonant,\n","                         device)\n","def output_transform(output):\n","    metric, pred_y, y = output\n","    return pred_y.cpu(), y.cpu()\n","EpochMetric(\n","    compute_fn=macro_recall,\n","    output_transform=output_transform\n",").attach(trainer, 'recall')\n","\n","pbar = ProgressBar()\n","pbar.attach(trainer, metric_names='all')\n","\n","evaluator = create_evaluator(classifier, device)\n","EpochMetric(\n","    compute_fn=macro_recall,\n","    output_transform=output_transform\n",").attach(evaluator, 'recall')\n","\n","def run_evaluator(engine):\n","    evaluator.run(valid_loader)\n","    schedule_lr(evaluator)\n","def schedule_lr(engine):\n","    # metrics = evaluator.state.metrics\n","    metrics = engine.state.metrics\n","    avg_mae = metrics['loss']\n","\n","    # --- update lr ---\n","    lr = scheduler.optimizer.param_groups[0]['lr']\n","    scheduler.step(avg_mae)\n","    scheduler_centloss_grapheme.step(avg_mae)\n","    scheduler_centloss_vowel.step(avg_mae)\n","    scheduler_centloss_consonant.step(avg_mae)\n","    log_report.report('lr', lr)\n","\n","trainer.add_event_handler(Events.EPOCH_COMPLETED, run_evaluator) #1epoch終わるごとにrun_evaluatorを実行\n","#trainer.add_event_handler(Events.EPOCH_COMPLETED, schedule_lr)\n","log_report = LogReport(evaluator, outdir)\n","trainer.add_event_handler(Events.EPOCH_COMPLETED, log_report)\n","trainer.add_event_handler(\n","    Events.EPOCH_COMPLETED,\n","    ModelSnapshotHandler(predictor, filepath=outdir +saved_model_name))\n"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<ignite.engine.engine.RemovableEventHandle at 0x7f46a3f306a0>"]},"metadata":{"tags":[]},"execution_count":57}]},{"cell_type":"code","metadata":{"scrolled":true,"id":"rfKBQxTvvvNM","colab_type":"code","outputId":"d92cedb1-5a4c-4ca4-f95f-75d945b8fbe1","colab":{"base_uri":"https://localhost:8080/"}},"source":["trainer.run(train_loader, max_epochs=300)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3d4ce9cb19374e6690ab4ae00c52514a","version_minor":0,"version_major":2},"text/plain":["HBox(children=(IntProgress(value=0, max=2511), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\repoch      1 train/loss  0.541495 train/loss_grapheme  0.781728 train/loss_vowel  0.363504 train/loss_consonant  0.239023 train/acc_grapheme  0.000000 train/acc_vowel  0.000000 train/acc_consonant  0.000000 train/recall  0.522978 valid/loss  0.190197 valid/loss_grapheme  0.242099 valid/loss_vowel  0.172534 valid/loss_consonant  0.104056 valid/acc_grapheme  0.949184 valid/acc_vowel  0.984674 valid/acc_consonant  0.984151 valid/recall  0.960460 lr  0.001000 elapsed_time  7805.033967 \n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"46c8cbf4c0a0426eb9936179505d1ce6","version_minor":0,"version_major":2},"text/plain":["HBox(children=(IntProgress(value=0, max=2511), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\repoch      2 train/loss  0.517654 train/loss_grapheme  0.742787 train/loss_vowel  0.351642 train/loss_consonant  0.233399 train/acc_grapheme  0.000000 train/acc_vowel  0.000000 train/acc_consonant  0.000000 train/recall  0.550308 valid/loss  0.171480 valid/loss_grapheme  0.210478 valid/loss_vowel  0.147284 valid/loss_consonant  0.117679 valid/acc_grapheme  0.950921 valid/acc_vowel  0.985047 valid/acc_consonant  0.983952 valid/recall  0.960094 lr  0.001000 elapsed_time  15638.595547 \n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3ab63f0e037244918d0b7f557bb4e91e","version_minor":0,"version_major":2},"text/plain":["HBox(children=(IntProgress(value=0, max=2511), HTML(value='')))"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"CpT7UEDIvvNR","colab_type":"code","colab":{}},"source":["train_history = log_report.get_dataframe()\n","train_history.to_csv(outdir+saved_model_name[:-3]+'.csv', index=False)\n","\n","train_history"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"voSVu9yavvNa","colab_type":"code","colab":{}},"source":["import requests\n","\n","line_notify_token = 'DDKITNfYZlyLUyV9sIZuXte9ms2SuW2cOOMdSPW9nlR'\n","line_notify_api = 'https://notify-api.line.me/api/notify'\n","message = 'owari'\n","\n","\n","payload = {'message': message}\n","headers = {'Authorization': 'Bearer ' + line_notify_token}  # 発行したトークン\n","line_notify = requests.post(line_notify_api, data=payload, headers=headers)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Dl9AVuYVvvNe","colab_type":"text"},"source":["<a id=\"pred\"></a>\n","# Prediction\n","\n","Please refer **[Bengali: SEResNeXt prediction with pytorch](https://www.kaggle.com/corochann/bengali-seresnext-prediction-with-pytorch)** for the prediction with trained model and submission for this competition!!!"]},{"cell_type":"markdown","metadata":{"id":"qtTlLAljvvNg","colab_type":"text"},"source":["<a id=\"ref\"></a>\n","# Reference and further reading\n","\n","#### Kernel\n","\n","**[Bangali.AI super fast data loading with feather](https://www.kaggle.com/corochann/bangali-ai-super-fast-data-loading-with-feather)**<br>\n","Simple example of how use feather format data to load data faster.\n","\n","**[Bengali: albumentations data augmentation tutorial](https://www.kaggle.com/corochann/bengali-albumentations-data-augmentation-tutorial)**<br>\n","Tutorial for Data augmentations with albumentations library.\n","\n","**[Bengali: SEResNeXt prediction with pytorch](https://www.kaggle.com/corochann/bengali-seresnext-prediction-with-pytorch)**<br>\n","**Prediction code of this kernel's trained model, please check this too!**\n","\n","**[Deep learning - CNN with Chainer: LB 0.99700](https://www.kaggle.com/corochann/deep-learning-cnn-with-chainer-lb-0-99700)**<br>\n","Data augmentation idea is based on this kernel, which achieves quite high accuracy on MNIST task.\n","\n","#### Dataset\n","**[bengaliai-cv19-feather](https://www.kaggle.com/corochann/bengaliaicv19feather)**<br>\n","Feather format dataset\n","\n","**[bengaliaicv19_seresnext101_32x4d](https://www.kaggle.com/corochann/bengaliaicv19-seresnext101-32x4d)**<br>\n","**Trained model weight with this kernel(v1)**\n","\n","**[bengaliaicv19_trainedmodels](https://www.kaggle.com/corochann/bengaliaicv19-trainedmodels)**<br>\n","**Trained model weight with this kernel(v2~)**\n","\n","#### Library\n","**https://github.com/pytorch/ignite**\n","\n","Used for training code abstraction. The advantage of abstracting the code is that we can re-use implemented handler class for other training, other competition.<br>\n","You don't need to write code for saving models, logging training loss/metric, show progressbar etc.\n","\n","**https://github.com/Cadene/pretrained-models.pytorch**\n","\n","Many pretrained models are supported by this library, and we can switch to use them easily.\n","Other model may perform better in this competition.\n","\n","**https://github.com/albumentations-team/albumentations**\n","\n","fast image augmentation library and easy to use wrapper around other libraries https://arxiv.org/abs/1809.06839<br>\n","I could not show all the methods, you can find more methods in the library, check yourself!"]},{"cell_type":"markdown","metadata":{"id":"Mymc1ZzwvvNh","colab_type":"text"},"source":["<h3 style=\"color:red\">If this kernel helps you, please upvote to keep me motivated :)<br>Thanks!</h3>"]}]}